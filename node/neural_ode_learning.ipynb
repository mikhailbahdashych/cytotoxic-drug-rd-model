{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d77bbda",
   "metadata": {},
   "source": [
    "# Neural ODE Learning for Cytotoxic Drug R&D Model\n",
    "\n",
    "**Task**: Learn the dynamics of the 4-variable PDE system (S, R, I, C) using Neural ODEs.\n",
    "Train neural networks to approximate dx/dt = f_θ(x, t) from PDE-generated trajectories.\n",
    "\n",
    "## Structure:\n",
    "- Section 1: Imports and Setup\n",
    "- Section 2: Data Generation from PDE\n",
    "- Section 3: Dataset Class and DataLoader\n",
    "- Section 4: Neural ODE Architectures (Small MLP, Large MLP)\n",
    "- Section 5: Training Functions\n",
    "- Section 6: Evaluation and Metrics\n",
    "- Section 7: Visualization\n",
    "- Section 8: Main Execution\n",
    "\n",
    "**Author**: Generated for ISZ Project\n",
    "**Date**: 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b473d5a4",
   "metadata": {},
   "source": [
    "## Section 1: Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fc2477",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import qmc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Neural ODE libraries\n",
    "USE_PNODE = False  # Set to False to use torchdiffeq as fallback\n",
    "\n",
    "try:\n",
    "    if USE_PNODE:\n",
    "        from pnode import petsc_adjoint\n",
    "        print(\"[OK] pnode imported successfully\")\n",
    "        PNODE_AVAILABLE = True\n",
    "    else:\n",
    "        raise ImportError(\"USE_PNODE is False, using fallback\")\n",
    "except ImportError as e:\n",
    "    print(f\"[WARNING] pnode not available ({e}), falling back to torchdiffeq\")\n",
    "    try:\n",
    "        from torchdiffeq import odeint_adjoint\n",
    "        print(\"[OK] torchdiffeq imported successfully\")\n",
    "        PNODE_AVAILABLE = False\n",
    "    except ImportError:\n",
    "        print(\"[ERROR] Neither pnode nor torchdiffeq available!\")\n",
    "        print(\"Install one with: uv pip install git+https://github.com/caidao22/pnode.git\")\n",
    "        print(\"Or: uv pip install torchdiffeq\")\n",
    "        sys.exit(1)\n",
    "\n",
    "# Import PDE module\n",
    "import importlib.util\n",
    "\n",
    "# Try to find the PDE module in the root directory\n",
    "pde_module_path = Path(\"2_tumor_diffusion_pde_analysis.py\")\n",
    "if not pde_module_path.exists():\n",
    "    # If not found, try parent directory (for backward compatibility)\n",
    "    pde_module_path = Path(\"../2_tumor_diffusion_pde_analysis.py\")\n",
    "    if not pde_module_path.exists():\n",
    "        print(\"[ERROR] Cannot find 2_tumor_diffusion_pde_analysis.py\")\n",
    "        print(\"Looked in:\")\n",
    "        print(\"  - ./2_tumor_diffusion_pde_analysis.py (root)\")\n",
    "        print(\"  - ../2_tumor_diffusion_pde_analysis.py (parent)\")\n",
    "        sys.exit(1)\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\n",
    "    \"pde_module\", str(pde_module_path.resolve())\n",
    ")\n",
    "pde_module = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"pde_module\"] = pde_module\n",
    "try:\n",
    "    spec.loader.exec_module(pde_module)\n",
    "    print(f\"[OK] PDE module imported from {pde_module_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to import PDE module: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Import needed components from PDE module\n",
    "Grid = pde_module.Grid\n",
    "Params = pde_module.Params\n",
    "init_fields = pde_module.init_fields\n",
    "\n",
    "# Setup directories - use node/ prefix when running from root\n",
    "# Detect if we're in the root directory or node directory\n",
    "if Path(\"node\").exists() and Path(\"node\").is_dir():\n",
    "    # Running from root directory\n",
    "    OUT_BASE = Path(\"node/out\")\n",
    "    FIG_BASE = Path(\"node/figs\")\n",
    "else:\n",
    "    # Running from node directory\n",
    "    OUT_BASE = Path(\"out\")\n",
    "    FIG_BASE = Path(\"figs\")\n",
    "\n",
    "# Create directories\n",
    "(OUT_BASE / \"neural_ode_dataset\").mkdir(parents=True, exist_ok=True)\n",
    "(OUT_BASE / \"neural_ode_models\").mkdir(parents=True, exist_ok=True)\n",
    "(OUT_BASE / \"neural_ode_results\").mkdir(parents=True, exist_ok=True)\n",
    "FIG_BASE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"[Device] Using: {device}\")\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Helper functions\n",
    "def save_json(data, path):\n",
    "    \"\"\"Save dictionary to JSON file\"\"\"\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    print(f\"[Saved] {path}\")\n",
    "\n",
    "def load_json(path):\n",
    "    \"\"\"Load JSON file to dictionary\"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def savefig(path, dpi=160):\n",
    "    \"\"\"Save matplotlib figure\"\"\"\n",
    "    plt.savefig(path, dpi=dpi, bbox_inches='tight')\n",
    "    print(f\"[Saved] {path}\")\n",
    "\n",
    "def rmse(pred, true):\n",
    "    \"\"\"Root mean squared error\"\"\"\n",
    "    return np.sqrt(np.mean((pred - true) ** 2))\n",
    "\n",
    "def mae(pred, true):\n",
    "    \"\"\"Mean absolute error\"\"\"\n",
    "    return np.mean(np.abs(pred - true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e32605",
   "metadata": {},
   "source": [
    "## Section 2: Data Generation from PDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f07c307",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_pde_simulation_for_node(grid, params, T=6.0, dt=0.01, save_every=10, custom_init=None):\n",
    "    \"\"\"\n",
    "    Run PDE simulation and extract spatially-averaged trajectory for Neural ODE training.\n",
    "\n",
    "    Args:\n",
    "        grid: Grid object from PDE module\n",
    "        params: Params object with model parameters\n",
    "        T: Final time\n",
    "        dt: Time step\n",
    "        save_every: Save frequency\n",
    "        custom_init: Optional tuple (S, R, I, C) for custom initial conditions\n",
    "\n",
    "    Returns:\n",
    "        dict with keys: t (time array), state (spatially averaged [S̄, R̄, Ī, C̄]), TB (tumor burden)\n",
    "    \"\"\"\n",
    "    # Initialize fields\n",
    "    if custom_init is not None:\n",
    "        S, R, I, C = custom_init\n",
    "    else:\n",
    "        S, R, I, C = init_fields(grid)\n",
    "\n",
    "    # Get solver function\n",
    "    use_semi_implicit = hasattr(pde_module, 'step_semi_implicit')\n",
    "    if use_semi_implicit:\n",
    "        step_func = pde_module.step_semi_implicit\n",
    "        print(\"[PDE] Using semi-implicit solver\")\n",
    "    else:\n",
    "        step_func = pde_module.step_explicit\n",
    "        print(\"[PDE] Using explicit solver\")\n",
    "\n",
    "    # Time stepping\n",
    "    n_steps = int(T / dt)\n",
    "    t_current = 0.0\n",
    "\n",
    "    # Storage for trajectory\n",
    "    trajectory = {\n",
    "        't': [],\n",
    "        'state': [],  # [S_mean, R_mean, I_mean, C_mean]\n",
    "        'TB': []      # Tumor burden\n",
    "    }\n",
    "\n",
    "    # Save initial condition\n",
    "    dx, dy = grid.dx, grid.dy\n",
    "    S_mean = np.mean(S)\n",
    "    R_mean = np.mean(R)\n",
    "    I_mean = np.mean(I)\n",
    "    C_mean = np.mean(C)\n",
    "    TB = np.sum(S + R) * dx * dy\n",
    "\n",
    "    trajectory['t'].append(t_current)\n",
    "    trajectory['state'].append([S_mean, R_mean, I_mean, C_mean])\n",
    "    trajectory['TB'].append(TB)\n",
    "\n",
    "    # Time integration\n",
    "    for step in range(n_steps):\n",
    "        # Take PDE step\n",
    "        if use_semi_implicit:\n",
    "            # semi_implicit needs system matrices - check if available\n",
    "            if hasattr(pde_module, 'precompute_system_matrices'):\n",
    "                sys_mats = pde_module.precompute_system_matrices(grid, params, dt)\n",
    "                S, R, I, C = step_func(S, R, I, C, grid, dt, params, sys_mats)\n",
    "            else:\n",
    "                # Fallback to explicit if no system matrices\n",
    "                S, R, I, C = pde_module.step_explicit(S, R, I, C, grid, dt, params)\n",
    "        else:\n",
    "            S, R, I, C = step_func(S, R, I, C, grid, dt, params)\n",
    "        t_current += dt\n",
    "\n",
    "        # Save at specified frequency\n",
    "        if (step + 1) % save_every == 0:\n",
    "            S_mean = np.mean(S)\n",
    "            R_mean = np.mean(R)\n",
    "            I_mean = np.mean(I)\n",
    "            C_mean = np.mean(C)\n",
    "            TB = np.sum(S + R) * dx * dy\n",
    "\n",
    "            trajectory['t'].append(t_current)\n",
    "            trajectory['state'].append([S_mean, R_mean, I_mean, C_mean])\n",
    "            trajectory['TB'].append(TB)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    trajectory['t'] = np.array(trajectory['t'])\n",
    "    trajectory['state'] = np.array(trajectory['state'])  # Shape: [n_timepoints, 4]\n",
    "    trajectory['TB'] = np.array(trajectory['TB'])\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "def generate_parameter_samples(n_samples=50, seed=42):\n",
    "    \"\"\"\n",
    "    Generate parameter samples using Latin Hypercube Sampling.\n",
    "\n",
    "    Samples 4 key therapy parameters:\n",
    "    - infusion_rate: [0.05, 0.25]\n",
    "    - alpha_S (drug efficacy on sensitive cells): [0.6, 1.0]\n",
    "    - mu_max (resistance induction rate): [0.02, 0.08]\n",
    "    - lam (drug clearance): [0.1, 0.3]\n",
    "\n",
    "    Returns:\n",
    "        List of Params objects\n",
    "    \"\"\"\n",
    "    sampler = qmc.LatinHypercube(d=4, seed=seed)\n",
    "    samples = sampler.random(n=n_samples)\n",
    "\n",
    "    params_list = []\n",
    "    for sample in samples:\n",
    "        p = Params()\n",
    "        # Use continuous infusion dosing\n",
    "        p.dose_type = \"infusion_const\"\n",
    "        p.infusion_rate = 0.05 + sample[0] * 0.20    # [0.05, 0.25]\n",
    "        p.alpha_S = 0.6 + sample[1] * 0.4             # [0.6, 1.0]\n",
    "        p.mu_max = 0.02 + sample[2] * 0.06            # [0.02, 0.08]\n",
    "        p.lam = 0.1 + sample[3] * 0.2                 # [0.1, 0.3]\n",
    "        params_list.append(p)\n",
    "\n",
    "    return params_list\n",
    "\n",
    "\n",
    "def generate_initial_conditions(grid, seed=None):\n",
    "    \"\"\"\n",
    "    Generate varied initial conditions for S and R.\n",
    "\n",
    "    Varies:\n",
    "    - Tumor center position: (x0, y0) ∈ [0.3, 0.7]²\n",
    "    - Tumor width: σ ∈ [0.10, 0.20]\n",
    "    - Initial magnitudes: S0 ∈ [0.4, 0.6], R0 ∈ [0.05, 0.15]\n",
    "\n",
    "    Returns:\n",
    "        S, R, I, C arrays\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    Nx, Ny = grid.Nx, grid.Ny\n",
    "    x = np.linspace(0, grid.Lx, Nx)\n",
    "    y = np.linspace(0, grid.Ly, Ny)\n",
    "    X, Y = np.meshgrid(x, y, indexing='ij')\n",
    "\n",
    "    # Random tumor center and width\n",
    "    x0 = 0.3 + np.random.rand() * 0.4\n",
    "    y0 = 0.3 + np.random.rand() * 0.4\n",
    "    sigma = 0.10 + np.random.rand() * 0.10\n",
    "\n",
    "    # Random initial magnitudes\n",
    "    S0_mag = 0.4 + np.random.rand() * 0.2\n",
    "    R0_mag = 0.05 + np.random.rand() * 0.10\n",
    "\n",
    "    # Create Gaussian distributions\n",
    "    r_squared = (X - x0)**2 + (Y - y0)**2\n",
    "    S = S0_mag * np.exp(-r_squared / (2 * sigma**2))\n",
    "\n",
    "    # Resistant cells in a ring\n",
    "    r_ring = 0.15 + np.random.rand() * 0.05\n",
    "    sigma_ring = 0.05 + np.random.rand() * 0.03\n",
    "    R = R0_mag * np.exp(-((np.sqrt(r_squared) - r_ring)**2) / (2 * sigma_ring**2))\n",
    "\n",
    "    # Immune and drug concentrations (fixed)\n",
    "    I = np.full((Nx, Ny), 0.02)\n",
    "    C = np.zeros((Nx, Ny))\n",
    "\n",
    "    return S, R, I, C\n",
    "\n",
    "\n",
    "def generate_neural_ode_dataset(n_trajectories=50, T=6.0, N_grid=64, verbose=True):\n",
    "    \"\"\"\n",
    "    Generate complete Neural ODE dataset with varied parameters and initial conditions.\n",
    "\n",
    "    Args:\n",
    "        n_trajectories: Number of trajectories to generate\n",
    "        T: Time horizon\n",
    "        N_grid: Grid resolution (NxN)\n",
    "        verbose: Print progress\n",
    "\n",
    "    Returns:\n",
    "        List of trajectory dictionaries\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Generating Neural ODE Dataset\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Number of trajectories: {n_trajectories}\")\n",
    "    print(f\"Time horizon: T = {T}\")\n",
    "    print(f\"Grid resolution: {N_grid} × {N_grid}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    # Setup\n",
    "    grid = Grid(Nx=N_grid, Ny=N_grid, Lx=1.0, Ly=1.0)\n",
    "    params_list = generate_parameter_samples(n_trajectories, seed=42)\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    # Generate trajectories\n",
    "    iterator = tqdm(range(n_trajectories), desc=\"Generating trajectories\") if verbose else range(n_trajectories)\n",
    "\n",
    "    for i in iterator:\n",
    "        # Get parameters\n",
    "        params = params_list[i]\n",
    "\n",
    "        # Generate initial conditions\n",
    "        S, R, I, C = generate_initial_conditions(grid, seed=i+100)\n",
    "\n",
    "        # Run PDE simulation with custom initial conditions\n",
    "        try:\n",
    "            trajectory = run_pde_simulation_for_node(\n",
    "                grid, params, T=T, dt=0.01, save_every=10,\n",
    "                custom_init=(S, R, I, C)\n",
    "            )\n",
    "\n",
    "            # Store trajectory with metadata\n",
    "            dataset.append({\n",
    "                'trajectory_id': i,\n",
    "                'params': asdict(params),\n",
    "                't': trajectory['t'],\n",
    "                'state': trajectory['state'],\n",
    "                'TB': trajectory['TB']\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"\\n[ERROR] Failed to generate trajectory {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\n[Success] Generated {len(dataset)} trajectories\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def save_dataset(dataset, train_frac=0.8, val_frac=0.1, suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Split and save dataset to disk.\n",
    "\n",
    "    Args:\n",
    "        dataset: List of trajectory dicts\n",
    "        train_frac: Fraction for training\n",
    "        val_frac: Fraction for validation (remainder goes to test)\n",
    "        suffix: Optional suffix to add to filenames (e.g., \"_run1\", \"_experiment_A\")\n",
    "    \"\"\"\n",
    "    n_total = len(dataset)\n",
    "    n_train = int(n_total * train_frac)\n",
    "    n_val = int(n_total * val_frac)\n",
    "    n_test = n_total - n_train - n_val\n",
    "\n",
    "    # Split\n",
    "    train_data = dataset[:n_train]\n",
    "    val_data = dataset[n_train:n_train+n_val]\n",
    "    test_data = dataset[n_train+n_val:]\n",
    "\n",
    "    # Add suffix to filenames\n",
    "    suffix_str = f\"_{suffix}\" if suffix else \"\"\n",
    "\n",
    "    # Save\n",
    "    np.savez_compressed(OUT_BASE / f\"neural_ode_dataset/train_trajectories{suffix_str}.npz\", trajectories=train_data)\n",
    "    np.savez_compressed(OUT_BASE / f\"neural_ode_dataset/val_trajectories{suffix_str}.npz\", trajectories=val_data)\n",
    "    np.savez_compressed(OUT_BASE / f\"neural_ode_dataset/test_trajectories{suffix_str}.npz\", trajectories=test_data)\n",
    "\n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'n_total': n_total,\n",
    "        'n_train': n_train,\n",
    "        'n_val': n_val,\n",
    "        'n_test': n_test,\n",
    "        'split': {'train': train_frac, 'val': val_frac, 'test': 1-train_frac-val_frac},\n",
    "        'suffix': suffix\n",
    "    }\n",
    "    save_json(metadata, OUT_BASE / f\"neural_ode_dataset/metadata{suffix_str}.json\")\n",
    "\n",
    "    print(f\"\\n[Dataset Split]\")\n",
    "    print(f\"  Train: {n_train} trajectories\")\n",
    "    print(f\"  Val:   {n_val} trajectories\")\n",
    "    print(f\"  Test:  {n_test} trajectories\")\n",
    "    print(f\"  Total: {n_total} trajectories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21403f7",
   "metadata": {},
   "source": [
    "## Section 3: Dataset Class and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccfa560",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class NODEDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for Neural ODE training.\n",
    "\n",
    "    Returns:\n",
    "        x0: Initial state [4]\n",
    "        t: Time points [n_timepoints]\n",
    "        states: Full trajectory [n_timepoints, 4]\n",
    "    \"\"\"\n",
    "    def __init__(self, trajectories, noise_level=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trajectories: List of trajectory dicts\n",
    "            noise_level: Standard deviation of Gaussian noise to add (0 = no noise)\n",
    "        \"\"\"\n",
    "        self.trajectories = trajectories\n",
    "        self.noise_level = noise_level\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        traj = self.trajectories[idx]\n",
    "\n",
    "        t = torch.tensor(traj['t'], dtype=torch.float32)\n",
    "        state = torch.tensor(traj['state'], dtype=torch.float32)\n",
    "\n",
    "        # Add optional noise\n",
    "        if self.noise_level > 0:\n",
    "            noise = torch.randn_like(state) * self.noise_level\n",
    "            state = state + noise\n",
    "            # Clamp to non-negative\n",
    "            state = torch.clamp(state, min=0.0)\n",
    "\n",
    "        x0 = state[0]  # Initial condition\n",
    "\n",
    "        return x0, t, state\n",
    "\n",
    "\n",
    "def load_dataset_splits(noise_level=0.0, suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Load train/val/test datasets from disk.\n",
    "\n",
    "    Args:\n",
    "        noise_level: Noise level to add to training data\n",
    "        suffix: Optional suffix matching the saved dataset files\n",
    "\n",
    "    Returns:\n",
    "        train_dataset, val_dataset, test_dataset\n",
    "    \"\"\"\n",
    "    suffix_str = f\"_{suffix}\" if suffix else \"\"\n",
    "    train_data = np.load(OUT_BASE / f\"neural_ode_dataset/train_trajectories{suffix_str}.npz\", allow_pickle=True)['trajectories']\n",
    "    val_data = np.load(OUT_BASE / f\"neural_ode_dataset/val_trajectories{suffix_str}.npz\", allow_pickle=True)['trajectories']\n",
    "    test_data = np.load(OUT_BASE / f\"neural_ode_dataset/test_trajectories{suffix_str}.npz\", allow_pickle=True)['trajectories']\n",
    "\n",
    "    train_dataset = NODEDataset(train_data, noise_level=noise_level)\n",
    "    val_dataset = NODEDataset(val_data, noise_level=0.0)  # No noise for validation\n",
    "    test_dataset = NODEDataset(test_data, noise_level=0.0)  # No noise for test\n",
    "\n",
    "    print(f\"[Datasets Loaded]\")\n",
    "    print(f\"  Train: {len(train_dataset)} trajectories\")\n",
    "    print(f\"  Val:   {len(val_dataset)} trajectories\")\n",
    "    print(f\"  Test:  {len(test_dataset)} trajectories\")\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e659e80e",
   "metadata": {},
   "source": [
    "## Section 4: Neural ODE Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492897a7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SmallMLP_NODE(nn.Module):\n",
    "    \"\"\"\n",
    "    Small MLP Neural ODE architecture (Architecture A).\n",
    "\n",
    "    - 2 hidden layers\n",
    "    - 32 neurons per layer\n",
    "    - Tanh activation\n",
    "    - Direct time encoding\n",
    "    - ~1,300 parameters\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim=4, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + 1, hidden_dim),  # +1 for time\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, state_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        \"\"\"\n",
    "        Forward pass: compute dx/dt = f(x, t).\n",
    "\n",
    "        Args:\n",
    "            t: Time (scalar or tensor)\n",
    "            x: State [batch_size, state_dim]\n",
    "\n",
    "        Returns:\n",
    "            dx/dt: [batch_size, state_dim]\n",
    "        \"\"\"\n",
    "        # Ensure t is a scalar tensor\n",
    "        if isinstance(t, (int, float)):\n",
    "            t = torch.tensor(t, dtype=torch.float32, device=x.device)\n",
    "\n",
    "        # Append time to state\n",
    "        t_vec = t * torch.ones(x.shape[0], 1, device=x.device, dtype=x.dtype)\n",
    "        x_with_t = torch.cat([x, t_vec], dim=1)\n",
    "\n",
    "        return self.net(x_with_t)\n",
    "\n",
    "\n",
    "class LargeMLP_NODE(nn.Module):\n",
    "    \"\"\"\n",
    "    Large MLP Neural ODE architecture with Fourier time encoding (Architecture B).\n",
    "\n",
    "    - 4 hidden layers (128 → 128 → 128 → 64)\n",
    "    - Tanh activation\n",
    "    - Fourier time encoding: [sin(ωt), cos(ωt)] where ω = 2π/5 (dosing period)\n",
    "    - ~33,000 parameters\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim=4, hidden_dim=128, dosing_period=5.0):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.omega = 2 * np.pi / dosing_period  # Angular frequency for Fourier encoding\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + 2, hidden_dim),  # +2 for sin/cos\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, state_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        \"\"\"\n",
    "        Forward pass with Fourier time encoding: compute dx/dt = f(x, sin(ωt), cos(ωt)).\n",
    "\n",
    "        Args:\n",
    "            t: Time (scalar or tensor)\n",
    "            x: State [batch_size, state_dim]\n",
    "\n",
    "        Returns:\n",
    "            dx/dt: [batch_size, state_dim]\n",
    "        \"\"\"\n",
    "        # Ensure t is a scalar tensor\n",
    "        if isinstance(t, (int, float)):\n",
    "            t = torch.tensor(t, dtype=torch.float32, device=x.device)\n",
    "\n",
    "        # Fourier time encoding\n",
    "        t_vec = t * torch.ones(x.shape[0], 1, device=x.device, dtype=x.dtype)\n",
    "        t_sin = torch.sin(self.omega * t_vec)\n",
    "        t_cos = torch.cos(self.omega * t_vec)\n",
    "\n",
    "        # Concatenate state with Fourier features\n",
    "        x_with_t = torch.cat([x, t_sin, t_cos], dim=1)\n",
    "\n",
    "        return self.net(x_with_t)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count total number of trainable parameters in model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d577c91",
   "metadata": {},
   "source": [
    "## Section 5: Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4712dcb6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def ode_solve(model, x0, t, method='rk4'):\n",
    "    \"\"\"\n",
    "    Solve ODE using appropriate library (pnode or torchdiffeq).\n",
    "\n",
    "    Args:\n",
    "        model: Neural ODE model\n",
    "        x0: Initial state [batch_size, state_dim]\n",
    "        t: Time points [n_timepoints]\n",
    "        method: ODE solver method\n",
    "\n",
    "    Returns:\n",
    "        Trajectory [n_timepoints, batch_size, state_dim]\n",
    "    \"\"\"\n",
    "    if PNODE_AVAILABLE:\n",
    "        # Use pnode\n",
    "        # Note: pnode has a different API, this is a simplified wrapper\n",
    "        # For production, would need proper pnode integration\n",
    "        # Fallback to Euler for simplicity if pnode API is complex\n",
    "        dt = t[1] - t[0]\n",
    "        trajectory = [x0]\n",
    "        x_current = x0\n",
    "        for i in range(len(t) - 1):\n",
    "            t_current = t[i]\n",
    "            dx = model(t_current, x_current)\n",
    "            x_next = x_current + dx * dt\n",
    "            trajectory.append(x_next)\n",
    "            x_current = x_next\n",
    "        return torch.stack(trajectory, dim=0)\n",
    "    else:\n",
    "        # Use torchdiffeq\n",
    "        return odeint_adjoint(model, x0, t, method=method)\n",
    "\n",
    "\n",
    "def compute_loss(model, x0_batch, t_batch, x_true_batch, loss_weights=None):\n",
    "    \"\"\"\n",
    "    Compute multi-component loss for Neural ODE training.\n",
    "\n",
    "    Loss = w1*L_trajectory + w2*L_endpoint + w3*L_physics\n",
    "\n",
    "    Args:\n",
    "        model: Neural ODE model\n",
    "        x0_batch: Initial states [batch_size, state_dim]\n",
    "        t_batch: Time points [n_timepoints]\n",
    "        x_true_batch: True trajectories [batch_size, n_timepoints, state_dim]\n",
    "        loss_weights: Dict with keys 'trajectory', 'endpoint', 'physics'\n",
    "\n",
    "    Returns:\n",
    "        total_loss, loss_dict\n",
    "    \"\"\"\n",
    "    if loss_weights is None:\n",
    "        loss_weights = {'trajectory': 1.0, 'endpoint': 5.0, 'physics': 0.1}\n",
    "\n",
    "    # Forward pass: integrate ODE\n",
    "    x_pred = ode_solve(model, x0_batch, t_batch, method='rk4')\n",
    "    x_pred = x_pred.permute(1, 0, 2)  # [batch, time, state]\n",
    "\n",
    "    # Trajectory loss: MSE over entire trajectory\n",
    "    loss_traj = torch.mean((x_pred - x_true_batch) ** 2)\n",
    "\n",
    "    # Endpoint loss: emphasize final state\n",
    "    loss_end = torch.mean((x_pred[:, -1, :] - x_true_batch[:, -1, :]) ** 2)\n",
    "\n",
    "    # Physics loss: enforce non-negativity\n",
    "    loss_phys = torch.mean(torch.relu(-x_pred) ** 2)\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = (\n",
    "        loss_weights['trajectory'] * loss_traj +\n",
    "        loss_weights['endpoint'] * loss_end +\n",
    "        loss_weights['physics'] * loss_phys\n",
    "    )\n",
    "\n",
    "    loss_dict = {\n",
    "        'total': total_loss.item(),\n",
    "        'trajectory': loss_traj.item(),\n",
    "        'endpoint': loss_end.item(),\n",
    "        'physics': loss_phys.item()\n",
    "    }\n",
    "\n",
    "    return total_loss, loss_dict\n",
    "\n",
    "\n",
    "def train_neural_ode(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=500,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5,\n",
    "    device='cpu',\n",
    "    save_path=None,\n",
    "    early_stopping_patience=50\n",
    "):\n",
    "    \"\"\"\n",
    "    Train Neural ODE model.\n",
    "\n",
    "    Args:\n",
    "        model: Neural ODE model\n",
    "        train_loader: Training DataLoader\n",
    "        val_loader: Validation DataLoader\n",
    "        num_epochs: Maximum number of epochs\n",
    "        lr: Learning rate\n",
    "        weight_decay: L2 regularization\n",
    "        device: Device to train on\n",
    "        save_path: Path to save best model\n",
    "        early_stopping_patience: Patience for early stopping\n",
    "\n",
    "    Returns:\n",
    "        model, history (dict with training/validation losses)\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=20\n",
    "    )\n",
    "\n",
    "    history = {\n",
    "        'epoch': [],\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'lr': []\n",
    "    }\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Neural ODE Model\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Epochs: {num_epochs}\")\n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    print(f\"Batch size: {train_loader.batch_size}\")\n",
    "    print(f\"Parameters: {count_parameters(model):,}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        for x0_batch, t_batch, x_true_batch in train_loader:\n",
    "            x0_batch = x0_batch.to(device)\n",
    "            t_batch = t_batch[0].to(device)  # Same time grid for all trajectories\n",
    "            x_true_batch = x_true_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute loss\n",
    "            loss, loss_dict = compute_loss(model, x0_batch, t_batch, x_true_batch)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x0_batch, t_batch, x_true_batch in val_loader:\n",
    "                x0_batch = x0_batch.to(device)\n",
    "                t_batch = t_batch[0].to(device)\n",
    "                x_true_batch = x_true_batch.to(device)\n",
    "\n",
    "                # Compute loss\n",
    "                loss, _ = compute_loss(model, x0_batch, t_batch, x_true_batch)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Record history\n",
    "        history['epoch'].append(epoch)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['lr'].append(current_lr)\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            if save_path:\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 10 == 0 or patience_counter == 0:\n",
    "            print(f\"Epoch {epoch:3d} | Train Loss: {avg_train_loss:.6f} | \"\n",
    "                  f\"Val Loss: {avg_val_loss:.6f} | LR: {current_lr:.6f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f\"\\n[Early Stopping] No improvement for {early_stopping_patience} epochs\")\n",
    "            print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\n[Training Complete]\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "\n",
    "    # Load best model\n",
    "    if save_path and os.path.exists(save_path):\n",
    "        model.load_state_dict(torch.load(save_path))\n",
    "        print(f\"[Loaded] Best model from {save_path}\")\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce37c108",
   "metadata": {},
   "source": [
    "## Section 6: Evaluation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f397eb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Evaluate Neural ODE model on test set.\n",
    "\n",
    "    Returns:\n",
    "        metrics: Dict with RMSE, MAE, etc.\n",
    "        predictions: List of prediction dicts\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    all_rmse_state = []\n",
    "    all_mae_state = []\n",
    "    all_rmse_tb = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x0_batch, t_batch, x_true_batch in test_loader:\n",
    "            x0_batch = x0_batch.to(device)\n",
    "            t_batch = t_batch[0].to(device)\n",
    "            x_true_batch = x_true_batch.to(device)\n",
    "\n",
    "            # Predict\n",
    "            x_pred = ode_solve(model, x0_batch, t_batch, method='rk4')\n",
    "            x_pred = x_pred.permute(1, 0, 2)  # [batch, time, state]\n",
    "\n",
    "            # Compute metrics\n",
    "            rmse_state = torch.sqrt(torch.mean((x_pred - x_true_batch) ** 2, dim=(1, 2)))\n",
    "            mae_state = torch.mean(torch.abs(x_pred - x_true_batch), dim=(1, 2))\n",
    "\n",
    "            # Tumor burden metrics (S + R)\n",
    "            TB_pred = x_pred[:, :, 0] + x_pred[:, :, 1]  # [batch, time]\n",
    "            TB_true = x_true_batch[:, :, 0] + x_true_batch[:, :, 1]\n",
    "            rmse_tb = torch.sqrt(torch.mean((TB_pred - TB_true) ** 2, dim=1))\n",
    "\n",
    "            all_rmse_state.extend(rmse_state.cpu().numpy())\n",
    "            all_mae_state.extend(mae_state.cpu().numpy())\n",
    "            all_rmse_tb.extend(rmse_tb.cpu().numpy())\n",
    "\n",
    "            # Store predictions\n",
    "            for i in range(x_pred.shape[0]):\n",
    "                predictions.append({\n",
    "                    't': t_batch.cpu().numpy(),\n",
    "                    'x_pred': x_pred[i].cpu().numpy(),\n",
    "                    'x_true': x_true_batch[i].cpu().numpy()\n",
    "                })\n",
    "\n",
    "    # Aggregate metrics\n",
    "    metrics = {\n",
    "        'RMSE_state': float(np.mean(all_rmse_state)),\n",
    "        'RMSE_state_std': float(np.std(all_rmse_state)),\n",
    "        'MAE_state': float(np.mean(all_mae_state)),\n",
    "        'MAE_state_std': float(np.std(all_mae_state)),\n",
    "        'RMSE_TB': float(np.mean(all_rmse_tb)),\n",
    "        'RMSE_TB_std': float(np.std(all_rmse_tb))\n",
    "    }\n",
    "\n",
    "    return metrics, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f0a4c9",
   "metadata": {},
   "source": [
    "## Section 7: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abd7701",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_data_samples(dataset, n_samples=16, save_path=None, save_data=True):\n",
    "    \"\"\"Plot sample trajectories from dataset\"\"\"\n",
    "    if save_path is None:\n",
    "        save_path = FIG_BASE / \"neural_ode_data_samples.png\"\n",
    "\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(16, 14))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    indices = np.random.choice(len(dataset), min(n_samples, len(dataset)), replace=False)\n",
    "\n",
    "    # Store data for later analysis\n",
    "    plot_data = {'indices': indices.tolist(), 'trajectories': []}\n",
    "\n",
    "    for idx, ax in enumerate(axes):\n",
    "        if idx < len(indices):\n",
    "            _, t, state = dataset[indices[idx]]\n",
    "            t = t.numpy()\n",
    "            state = state.numpy()\n",
    "\n",
    "            # Store data\n",
    "            plot_data['trajectories'].append({\n",
    "                'trajectory_id': int(indices[idx]),\n",
    "                't': t.tolist(),\n",
    "                'S': state[:, 0].tolist(),\n",
    "                'R': state[:, 1].tolist(),\n",
    "                'I': state[:, 2].tolist(),\n",
    "                'C': state[:, 3].tolist()\n",
    "            })\n",
    "\n",
    "            # Plot each state variable\n",
    "            ax.plot(t, state[:, 0], 'r-', label='S', linewidth=1.5)\n",
    "            ax.plot(t, state[:, 1], 'b-', label='R', linewidth=1.5)\n",
    "            ax.plot(t, state[:, 2], 'g-', label='I', linewidth=1.5)\n",
    "            ax.plot(t, state[:, 3], 'm-', label='C', linewidth=1.5)\n",
    "\n",
    "            ax.set_xlabel('Time t')\n",
    "            ax.set_ylabel('State')\n",
    "            ax.set_title(f'Trajectory {indices[idx]}')\n",
    "            if idx == 0:\n",
    "                ax.legend(loc='best', fontsize=8)\n",
    "            ax.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Save underlying data\n",
    "    if save_data:\n",
    "        data_path = str(save_path).replace('.png', '_data.json')\n",
    "        save_json(plot_data, data_path)\n",
    "\n",
    "\n",
    "def plot_training_curves(history_small, history_large, save_path=None, save_data=True):\n",
    "    \"\"\"Plot training and validation loss curves for both models\"\"\"\n",
    "    if save_path is None:\n",
    "        save_path = FIG_BASE / \"neural_ode_training_curves.png\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Small MLP\n",
    "    ax = axes[0]\n",
    "    ax.plot(history_small['epoch'], history_small['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "    ax.plot(history_small['epoch'], history_small['val_loss'], 'r-', label='Validation', linewidth=2)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Small MLP (2 layers, 32 neurons)')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    # Large MLP\n",
    "    ax = axes[1]\n",
    "    ax.plot(history_large['epoch'], history_large['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "    ax.plot(history_large['epoch'], history_large['val_loss'], 'r-', label='Validation', linewidth=2)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Large MLP (4 layers, 128 neurons)')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Save underlying data\n",
    "    if save_data:\n",
    "        plot_data = {\n",
    "            'small_mlp': history_small,\n",
    "            'large_mlp': history_large\n",
    "        }\n",
    "        data_path = str(save_path).replace('.png', '_data.json')\n",
    "        save_json(plot_data, data_path)\n",
    "\n",
    "\n",
    "def plot_test_predictions(predictions, model_name, save_path=None, save_data=True):\n",
    "    \"\"\"Plot predictions vs ground truth for first test trajectory\"\"\"\n",
    "    if save_path is None:\n",
    "        save_path = FIG_BASE / \"neural_ode_test_predictions.png\"\n",
    "    pred = predictions[0]\n",
    "    t = pred['t']\n",
    "    x_pred = pred['x_pred']\n",
    "    x_true = pred['x_true']\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    state_names = ['S (Sensitive)', 'R (Resistant)', 'I (Immune)', 'C (Drug)']\n",
    "    colors = ['red', 'blue', 'green', 'magenta']\n",
    "\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        ax.plot(t, x_true[:, i], color=colors[i], linestyle='-', linewidth=2.5,\n",
    "                label='PDE (Ground Truth)', alpha=0.8)\n",
    "        ax.plot(t, x_pred[:, i], color=colors[i], linestyle='--', linewidth=2,\n",
    "                label='Neural ODE', alpha=0.9)\n",
    "        ax.set_xlabel('Time t', fontsize=11)\n",
    "        ax.set_ylabel('State Value', fontsize=11)\n",
    "        ax.set_title(state_names[i], fontsize=12, fontweight='bold')\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "    plt.suptitle(f'Neural ODE Predictions vs PDE Ground Truth ({model_name})',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Save underlying data\n",
    "    if save_data:\n",
    "        plot_data = {\n",
    "            'model': model_name,\n",
    "            't': t.tolist(),\n",
    "            'predictions': {\n",
    "                'S': x_pred[:, 0].tolist(),\n",
    "                'R': x_pred[:, 1].tolist(),\n",
    "                'I': x_pred[:, 2].tolist(),\n",
    "                'C': x_pred[:, 3].tolist()\n",
    "            },\n",
    "            'ground_truth': {\n",
    "                'S': x_true[:, 0].tolist(),\n",
    "                'R': x_true[:, 1].tolist(),\n",
    "                'I': x_true[:, 2].tolist(),\n",
    "                'C': x_true[:, 3].tolist()\n",
    "            },\n",
    "            'errors': {\n",
    "                'S': (x_pred[:, 0] - x_true[:, 0]).tolist(),\n",
    "                'R': (x_pred[:, 1] - x_true[:, 1]).tolist(),\n",
    "                'I': (x_pred[:, 2] - x_true[:, 2]).tolist(),\n",
    "                'C': (x_pred[:, 3] - x_true[:, 3]).tolist()\n",
    "            }\n",
    "        }\n",
    "        data_path = str(save_path).replace('.png', '_data.json')\n",
    "        save_json(plot_data, data_path)\n",
    "\n",
    "\n",
    "def plot_architecture_comparison(metrics_small, metrics_large, save_path=None, save_data=True):\n",
    "    \"\"\"Bar chart comparing model architectures\"\"\"\n",
    "    if save_path is None:\n",
    "        save_path = FIG_BASE / \"neural_ode_architecture_comparison.png\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    models = ['Small MLP', 'Large MLP']\n",
    "\n",
    "    # RMSE_TB\n",
    "    ax = axes[0]\n",
    "    rmse_tb = [metrics_small['RMSE_TB'], metrics_large['RMSE_TB']]\n",
    "    rmse_tb_std = [metrics_small['RMSE_TB_std'], metrics_large['RMSE_TB_std']]\n",
    "    ax.bar(models, rmse_tb, yerr=rmse_tb_std, capsize=5, color=['skyblue', 'salmon'])\n",
    "    ax.set_ylabel('RMSE (Tumor Burden)')\n",
    "    ax.set_title('Prediction Error (Tumor Burden)')\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "    # RMSE_state\n",
    "    ax = axes[1]\n",
    "    rmse_state = [metrics_small['RMSE_state'], metrics_large['RMSE_state']]\n",
    "    rmse_state_std = [metrics_small['RMSE_state_std'], metrics_large['RMSE_state_std']]\n",
    "    ax.bar(models, rmse_state, yerr=rmse_state_std, capsize=5, color=['skyblue', 'salmon'])\n",
    "    ax.set_ylabel('RMSE (All States)')\n",
    "    ax.set_title('Prediction Error (All States)')\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "    # Parameter count\n",
    "    ax = axes[2]\n",
    "    param_counts = [1300, 33000]  # Approximate\n",
    "    ax.bar(models, param_counts, color=['skyblue', 'salmon'])\n",
    "    ax.set_ylabel('Number of Parameters')\n",
    "    ax.set_title('Model Complexity')\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Save underlying data\n",
    "    if save_data:\n",
    "        plot_data = {\n",
    "            'models': models,\n",
    "            'rmse_tumor_burden': {\n",
    "                'values': rmse_tb,\n",
    "                'std': rmse_tb_std\n",
    "            },\n",
    "            'rmse_all_states': {\n",
    "                'values': rmse_state,\n",
    "                'std': rmse_state_std\n",
    "            },\n",
    "            'parameter_counts': param_counts,\n",
    "            'metrics_small': metrics_small,\n",
    "            'metrics_large': metrics_large\n",
    "        }\n",
    "        data_path = str(save_path).replace('.png', '_data.json')\n",
    "        save_json(plot_data, data_path)\n",
    "\n",
    "\n",
    "def plot_extrapolation_test(model, test_dataset, device='cpu', save_path=None, save_data=True):\n",
    "    \"\"\"Test model extrapolation beyond training window\"\"\"\n",
    "    if save_path is None:\n",
    "        save_path = FIG_BASE / \"neural_ode_extrapolation.png\"\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Get first test trajectory\n",
    "    x0, t_train, x_true_train = test_dataset[0]\n",
    "    x0 = x0.unsqueeze(0).to(device)\n",
    "    t_train = t_train.to(device)\n",
    "\n",
    "    # Extend time to t=8 (training was t=6)\n",
    "    t_extended = torch.linspace(0, 8.0, 800).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_pred_extended = ode_solve(model, x0, t_extended, method='rk4')\n",
    "        x_pred_extended = x_pred_extended.squeeze(1).cpu().numpy()\n",
    "\n",
    "    t_extended_np = t_extended.cpu().numpy()\n",
    "    t_train_np = t_train.cpu().numpy()\n",
    "    x_true_train_np = x_true_train.cpu().numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    state_names = ['S (Sensitive)', 'R (Resistant)', 'I (Immune)', 'C (Drug)']\n",
    "    colors = ['red', 'blue', 'green', 'magenta']\n",
    "\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        # Training window\n",
    "        ax.plot(t_train_np, x_true_train_np[:, i], color=colors[i], linestyle='-',\n",
    "                linewidth=2.5, label='PDE (Training Window)', alpha=0.8)\n",
    "\n",
    "        # Neural ODE prediction (full)\n",
    "        ax.plot(t_extended_np, x_pred_extended[:, i], color=colors[i], linestyle='--',\n",
    "                linewidth=2, label='Neural ODE (Extended)', alpha=0.9)\n",
    "\n",
    "        # Mark training window boundary\n",
    "        ax.axvline(x=6.0, color='black', linestyle=':', linewidth=1.5, label='Training Boundary')\n",
    "\n",
    "        ax.set_xlabel('Time t', fontsize=11)\n",
    "        ax.set_ylabel('State Value', fontsize=11)\n",
    "        ax.set_title(state_names[i], fontsize=12, fontweight='bold')\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "    plt.suptitle('Extrapolation Test: Prediction Beyond Training Window',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Save underlying data\n",
    "    if save_data:\n",
    "        plot_data = {\n",
    "            't_training': t_train_np.tolist(),\n",
    "            't_extended': t_extended_np.tolist(),\n",
    "            'training_boundary': 6.0,\n",
    "            'ground_truth_training': {\n",
    "                'S': x_true_train_np[:, 0].tolist(),\n",
    "                'R': x_true_train_np[:, 1].tolist(),\n",
    "                'I': x_true_train_np[:, 2].tolist(),\n",
    "                'C': x_true_train_np[:, 3].tolist()\n",
    "            },\n",
    "            'predictions_extended': {\n",
    "                'S': x_pred_extended[:, 0].tolist(),\n",
    "                'R': x_pred_extended[:, 1].tolist(),\n",
    "                'I': x_pred_extended[:, 2].tolist(),\n",
    "                'C': x_pred_extended[:, 3].tolist()\n",
    "            }\n",
    "        }\n",
    "        data_path = str(save_path).replace('.png', '_data.json')\n",
    "        save_json(plot_data, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e46015",
   "metadata": {},
   "source": [
    "## Section 8: Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7643233b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_node(mode=None, model='small_mlp', epochs=500, batch_size=8, lr=1e-3, ts_type='rk4',\n",
    "             suffix=\"\", use_timestamp=False):\n",
    "    \"\"\"\n",
    "    Main execution function that works both from command line and notebook.\n",
    "\n",
    "    Args:\n",
    "        mode: Execution mode ('generate_data', 'train', 'evaluate', 'visualize')\n",
    "        model: Model architecture ('small_mlp', 'large_mlp')\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "        lr: Learning rate\n",
    "        ts_type: ODE solver type\n",
    "        suffix: Optional suffix to add to output files (e.g., \"run1\", \"experiment_A\")\n",
    "        use_timestamp: If True, automatically add timestamp to suffix\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate suffix with timestamp if requested\n",
    "    if use_timestamp:\n",
    "        from datetime import datetime\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        suffix = f\"{suffix}_{timestamp}\" if suffix else timestamp\n",
    "\n",
    "    # Create a simple args object for compatibility\n",
    "    class Args:\n",
    "        pass\n",
    "    args = Args()\n",
    "    args.mode = mode\n",
    "    args.model = model\n",
    "    args.epochs = epochs\n",
    "    args.batch_size = batch_size\n",
    "    args.lr = lr\n",
    "    args.ts_type = ts_type\n",
    "    args.suffix = suffix\n",
    "\n",
    "    # ========================================================================\n",
    "    # Mode: Generate Data\n",
    "    # ========================================================================\n",
    "    if args.mode == 'generate_data':\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MODE: Generate Data\")\n",
    "        if args.suffix:\n",
    "            print(f\"Suffix: {args.suffix}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        dataset = generate_neural_ode_dataset(n_trajectories=50, T=6.0, N_grid=64)\n",
    "        save_dataset(dataset, train_frac=0.8, val_frac=0.1, suffix=args.suffix)\n",
    "\n",
    "        # Visualize sample trajectories\n",
    "        train_dataset, _, _ = load_dataset_splits(suffix=args.suffix)\n",
    "        suffix_str = f\"_{args.suffix}\" if args.suffix else \"\"\n",
    "        plot_data_samples(train_dataset, n_samples=16,\n",
    "                         save_path=FIG_BASE / f\"neural_ode_data_samples{suffix_str}.png\")\n",
    "\n",
    "        print(\"\\n[Done] Data generation complete!\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Mode: Train\n",
    "    # ========================================================================\n",
    "    elif args.mode == 'train':\n",
    "        suffix_str = f\"_{args.suffix}\" if args.suffix else \"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"MODE: Train {args.model.upper()}\")\n",
    "        if args.suffix:\n",
    "            print(f\"Suffix: {args.suffix}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # Load datasets\n",
    "        train_dataset, val_dataset, _ = load_dataset_splits(noise_level=0.01, suffix=args.suffix)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "        # Create model\n",
    "        if args.model == 'small_mlp':\n",
    "            model = SmallMLP_NODE(state_dim=4, hidden_dim=32)\n",
    "        else:  # large_mlp\n",
    "            model = LargeMLP_NODE(state_dim=4, hidden_dim=128, dosing_period=5.0)\n",
    "\n",
    "        print(f\"\\n[Model] {args.model}\")\n",
    "        print(f\"Parameters: {count_parameters(model):,}\")\n",
    "\n",
    "        # Train\n",
    "        save_path = OUT_BASE / f\"neural_ode_models/model_{args.model}{suffix_str}.pt\"\n",
    "        model, history = train_neural_ode(\n",
    "            model, train_loader, val_loader,\n",
    "            num_epochs=args.epochs,\n",
    "            lr=args.lr,\n",
    "            device=device,\n",
    "            save_path=save_path,\n",
    "            early_stopping_patience=50\n",
    "        )\n",
    "\n",
    "        # Save training history\n",
    "        history_df = pd.DataFrame(history)\n",
    "        history_path = OUT_BASE / f\"neural_ode_models/training_history_{args.model}{suffix_str}.csv\"\n",
    "        history_df.to_csv(history_path, index=False)\n",
    "        print(f\"[Saved] Training history: {history_path}\")\n",
    "\n",
    "        print(\"\\n[Done] Training complete!\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Mode: Evaluate\n",
    "    # ========================================================================\n",
    "    elif args.mode == 'evaluate':\n",
    "        suffix_str = f\"_{args.suffix}\" if args.suffix else \"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"MODE: Evaluate {args.model.upper()}\")\n",
    "        if args.suffix:\n",
    "            print(f\"Suffix: {args.suffix}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # Load test dataset\n",
    "        _, _, test_dataset = load_dataset_splits(suffix=args.suffix)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "        # Load model\n",
    "        if args.model == 'small_mlp':\n",
    "            model = SmallMLP_NODE(state_dim=4, hidden_dim=32)\n",
    "        else:  # large_mlp\n",
    "            model = LargeMLP_NODE(state_dim=4, hidden_dim=128, dosing_period=5.0)\n",
    "\n",
    "        model_path = OUT_BASE / f\"neural_ode_models/model_{args.model}{suffix_str}.pt\"\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print(f\"[Loaded] Model from {model_path}\")\n",
    "\n",
    "        # Evaluate\n",
    "        metrics, predictions = evaluate_model(model, test_loader, device=device)\n",
    "\n",
    "        # Print metrics\n",
    "        print(\"\\n[Metrics]\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"  {key}: {value:.6f}\")\n",
    "\n",
    "        # Save metrics\n",
    "        metrics_path = OUT_BASE / f\"neural_ode_results/metrics_{args.model}{suffix_str}.json\"\n",
    "        save_json(metrics, metrics_path)\n",
    "\n",
    "        # Plot predictions\n",
    "        plot_path = FIG_BASE / f\"neural_ode_test_predictions_{args.model}{suffix_str}.png\"\n",
    "        plot_test_predictions(predictions, args.model.upper(), save_path=plot_path)\n",
    "\n",
    "        print(\"\\n[Done] Evaluation complete!\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Mode: Visualize\n",
    "    # ========================================================================\n",
    "    elif args.mode == 'visualize':\n",
    "        suffix_str = f\"_{args.suffix}\" if args.suffix else \"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MODE: Visualize\")\n",
    "        if args.suffix:\n",
    "            print(f\"Suffix: {args.suffix}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # Load training histories\n",
    "        try:\n",
    "            history_small = pd.read_csv(OUT_BASE / f\"neural_ode_models/training_history_small_mlp{suffix_str}.csv\").to_dict('list')\n",
    "            history_large = pd.read_csv(OUT_BASE / f\"neural_ode_models/training_history_large_mlp{suffix_str}.csv\").to_dict('list')\n",
    "            plot_training_curves(history_small, history_large,\n",
    "                               save_path=FIG_BASE / f\"neural_ode_training_curves{suffix_str}.png\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"[Warning] Training histories not found, skipping training curves plot\")\n",
    "\n",
    "        # Load metrics\n",
    "        try:\n",
    "            metrics_small = load_json(OUT_BASE / f\"neural_ode_results/metrics_small_mlp{suffix_str}.json\")\n",
    "            metrics_large = load_json(OUT_BASE / f\"neural_ode_results/metrics_large_mlp{suffix_str}.json\")\n",
    "            plot_architecture_comparison(metrics_small, metrics_large,\n",
    "                                        save_path=FIG_BASE / f\"neural_ode_architecture_comparison{suffix_str}.png\")\n",
    "\n",
    "            # Create summary\n",
    "            metrics_summary = {\n",
    "                'small_mlp': metrics_small,\n",
    "                'large_mlp': metrics_large,\n",
    "                'suffix': args.suffix\n",
    "            }\n",
    "            save_json(metrics_summary, OUT_BASE / f\"neural_ode_results/metrics_summary{suffix_str}.json\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"[Warning] Metrics not found, skipping comparison plot\")\n",
    "\n",
    "        # Extrapolation test\n",
    "        try:\n",
    "            _, _, test_dataset = load_dataset_splits(suffix=args.suffix)\n",
    "\n",
    "            # Try large MLP first, fall back to small MLP if not available\n",
    "            large_mlp_path = OUT_BASE / f\"neural_ode_models/model_large_mlp{suffix_str}.pt\"\n",
    "            small_mlp_path = OUT_BASE / f\"neural_ode_models/model_small_mlp{suffix_str}.pt\"\n",
    "\n",
    "            if large_mlp_path.exists():\n",
    "                model = LargeMLP_NODE(state_dim=4, hidden_dim=128, dosing_period=5.0)\n",
    "                model.load_state_dict(torch.load(large_mlp_path))\n",
    "                model_name = \"large_mlp\"\n",
    "                print(\"[Extrapolation] Using Large MLP model\")\n",
    "            elif small_mlp_path.exists():\n",
    "                model = SmallMLP_NODE(state_dim=4, hidden_dim=32)\n",
    "                model.load_state_dict(torch.load(small_mlp_path))\n",
    "                model_name = \"small_mlp\"\n",
    "                print(\"[Extrapolation] Using Small MLP model (large_mlp not found)\")\n",
    "            else:\n",
    "                raise FileNotFoundError(\"No trained models found for extrapolation test\")\n",
    "\n",
    "            plot_extrapolation_test(model, test_dataset, device=device,\n",
    "                                   save_path=FIG_BASE / f\"neural_ode_extrapolation_{model_name}{suffix_str}.png\")\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] Extrapolation test failed: {e}\")\n",
    "\n",
    "        print(\"\\n[Done] Visualization complete!\")\n",
    "        print(\"\\nGenerated figures (depending on available models):\")\n",
    "        print(f\"  - figs/neural_ode_data_samples{suffix_str}.png (if dataset exists)\")\n",
    "        print(f\"  - figs/neural_ode_training_curves{suffix_str}.png (if both models trained)\")\n",
    "        print(f\"  - figs/neural_ode_test_predictions_small_mlp{suffix_str}.png (if small_mlp evaluated)\")\n",
    "        print(f\"  - figs/neural_ode_test_predictions_large_mlp{suffix_str}.png (if large_mlp evaluated)\")\n",
    "        print(f\"  - figs/neural_ode_architecture_comparison{suffix_str}.png (if both models evaluated)\")\n",
    "        print(f\"  - figs/neural_ode_extrapolation_*{suffix_str}.png (if any model trained)\")\n",
    "        print(f\"\\nData files (JSON) saved alongside each figure for analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997e439f",
   "metadata": {},
   "source": [
    "## Notebook Usage Examples\n",
    "\n",
    "When running in a Jupyter notebook, you can call the run_node function directly with parameters.\n",
    "\n",
    "### Using Suffixes to Organize Experiments\n",
    "\n",
    "You can use the `suffix` parameter to distinguish between different runs:\n",
    "- `suffix=\"run1\"` - Manual naming\n",
    "- `suffix=\"experiment_A\"` - Descriptive names\n",
    "- `use_timestamp=True` - Automatic timestamp (e.g., \"20250110_143022\")\n",
    "- Both: `suffix=\"exp1\", use_timestamp=True` - Combined (e.g., \"exp1_20250110_143022\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b85026",
   "metadata": {},
   "source": [
    "### Example 1: Generate Dataset (with suffix)\n",
    "```python\n",
    "# Uncomment to run:\n",
    "run_node(mode='generate_data', suffix='run1')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868762cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_node(mode='generate_data', suffix='small_mlp_run')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9380aa2",
   "metadata": {},
   "source": [
    "### Example 2: Train Both Models (with suffix)\n",
    "```python\n",
    "# Uncomment to run:\n",
    "# Train small MLP\n",
    "run_node(mode='train', model='small_mlp', epochs=500, batch_size=8, lr=1e-3, suffix='run1')\n",
    "\n",
    "# Train large MLP\n",
    "run_node(mode='train', model='large_mlp', epochs=500, batch_size=8, lr=1e-3, suffix='run1')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad6c949",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_node(mode='train', model='small_mlp', epochs=500, batch_size=128, lr=1e-3, suffix='small_mlp_run')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789e32b4",
   "metadata": {},
   "source": [
    "### Example 3: Evaluate Both Models\n",
    "```python\n",
    "# Uncomment to run:\n",
    "run_node(mode='evaluate', model='small_mlp', suffix='run1')\n",
    "run_node(mode='evaluate', model='large_mlp', suffix='run1')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae17101",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_node(mode='evaluate', model='small_mlp', suffix='small_mlp_run')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839cb478",
   "metadata": {},
   "source": [
    "### Example 4: Generate Visualizations\n",
    "```python\n",
    "# Uncomment to run:\n",
    "run_node(mode='visualize', suffix='run1')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5045ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_node(mode='visualize', suffix='small_mlp_run')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fb44b4",
   "metadata": {},
   "source": [
    "### Example 5: Complete Pipeline with Timestamp\n",
    "```python\n",
    "# Uncomment to run:\n",
    "# This will automatically add timestamp to all outputs\n",
    "run_node(mode='generate_data', use_timestamp=True)\n",
    "run_node(mode='train', model='small_mlp', epochs=500, use_timestamp=True)\n",
    "run_node(mode='train', model='large_mlp', epochs=500, use_timestamp=True)\n",
    "run_node(mode='evaluate', model='small_mlp', use_timestamp=True)\n",
    "run_node(mode='evaluate', model='large_mlp', use_timestamp=True)\n",
    "run_node(mode='visualize', use_timestamp=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424a3692",
   "metadata": {},
   "source": [
    "### Example 6: Compare Different Hyperparameters\n",
    "```python\n",
    "# Uncomment to run:\n",
    "# Run with different learning rates\n",
    "run_node(mode='train', model='small_mlp', epochs=300, lr=1e-3, suffix='lr_1e3')\n",
    "run_node(mode='train', model='small_mlp', epochs=300, lr=5e-4, suffix='lr_5e4')\n",
    "run_node(mode='train', model='small_mlp', epochs=300, lr=1e-4, suffix='lr_1e4')\n",
    "\n",
    "# Evaluate each\n",
    "run_node(mode='evaluate', model='small_mlp', suffix='lr_1e3')\n",
    "run_node(mode='evaluate', model='small_mlp', suffix='lr_5e4')\n",
    "run_node(mode='evaluate', model='small_mlp', suffix='lr_1e4')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f693e09b",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "source": [
    "### Output Files Structure\n",
    "\n",
    "With suffix enabled, your files will be organized like this:\n",
    "```\n",
    "node/out/\n",
    "  neural_ode_dataset/\n",
    "    train_trajectories_run1.npz\n",
    "    val_trajectories_run1.npz\n",
    "    test_trajectories_run1.npz\n",
    "  neural_ode_models/\n",
    "    model_small_mlp_run1.pt\n",
    "    model_large_mlp_run1.pt\n",
    "    training_history_small_mlp_run1.csv\n",
    "    training_history_large_mlp_run1.csv\n",
    "  neural_ode_results/\n",
    "    metrics_small_mlp_run1.json\n",
    "    metrics_large_mlp_run1.json\n",
    "\n",
    "node/figs/\n",
    "  neural_ode_data_samples_run1.png\n",
    "  neural_ode_data_samples_run1_data.json  # Raw plot data for analysis\n",
    "  neural_ode_training_curves_run1.png\n",
    "  neural_ode_training_curves_run1_data.json\n",
    "  neural_ode_test_predictions_small_mlp_run1.png\n",
    "  neural_ode_test_predictions_small_mlp_run1_data.json\n",
    "  ...\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
